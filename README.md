# üìä PR√ÅCTICA 2: PIPELINE DE PLN Y VECTORIZACI√ìN DE DOCUMENTOS
## Tecnolog√≠as de Lenguaje Natural

**Autor:** Escamilla Lazcano Sa√∫l
**Carrera:** Ingenier√≠a En Inteligencia Artificial

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![spaCy](https://img.shields.io/badge/Librer√≠a-spaCy-blue.svg)](https://spacy.io/)
[![NLTK](https://img.shields.io/badge/Librer√≠a-NLTK-green.svg)](https://www.nltk.org/)
[![Pandas](https://img.shields.io/badge/Librer√≠a-Pandas-purple.svg)](https://pandas.pydata.org/)
[![Matplotlib](https://img.shields.io/badge/Librer√≠a-WordCloud%20%7C%20Matplotlib-orange.svg)](https://matplotlib.org/)

## üöÄ Descripci√≥n del Proyecto

Este proyecto es un **pipeline completo de Procesamiento de Lenguaje Natural (PLN)** desarrollado en un Jupyter Notebook. El objetivo es tomar un corpus de texto crudo, aplicar un riguroso proceso de **normalizaci√≥n** para limpiarlo, y finalmente, **vectorizarlo** (convertirlo en n√∫meros) usando cuatro t√©cnicas fundamentales, incluyendo la implementaci√≥n de **TF-IDF** desde cero.

Este cuaderno demuestra el flujo de trabajo esencial para preparar datos de texto para cualquier modelo de Machine Learning.

---

## üìÇ 1. Corpus de Datos

El proyecto utiliza un corpus personalizado de **10 documentos** en ingl√©s.
* **Tema:** "L√≠neas de Carrera en Automovilismo" (*Racing Lines*).
* **Requisito:** Cada documento contiene m√°s de 15 tokens para asegurar un an√°lisis significativo.

---

## ‚öôÔ∏è 2. Pipeline de Normalizaci√≥n de Texto (Puntos 1 y 2)

El primer paso cr√≠tico en cualquier tarea de PLN es la **normalizaci√≥n** del texto. Este proceso limpia el "ruido" y estandariza las palabras para que el an√°lisis sea coherente y preciso.

Se implementaron y compararon 7 t√©cnicas de normalizaci√≥n diferentes:

| Inciso | Proceso de Normalizaci√≥n |
| :--- | :--- |
| **a** | **Preprocesamiento Base** (Min√∫sculas, sin *stopwords* ni puntuaci√≥n) |
| **b** | Base (a) + **Lematizaci√≥n Simple** (con `spaCy`) |
| **c** | Base (a) + **Stemming Simple** (con `NLTK SnowballStemmer`) |
| **d** | Base (a) ‚Üí Lematizaci√≥n ‚Üí Stemming |
| **e** | Base (a) ‚Üí Stemming ‚Üí Lematizaci√≥n |
| **f** | Base (a) + **POS-Tagging** ‚Üí Lematizaci√≥n (con `spaCy`) |
| **g** | Base (a) + **POS-Tagging** ‚Üí Stemming (con `NLTK`) |

### üî¨ Justificaci√≥n de la Normalizaci√≥n (Punto 2)

Para los pasos siguientes, se seleccion√≥ el **inciso (b) Lematizaci√≥n Simple** como el m√©todo de normalizaci√≥n definitivo.

**Justificaci√≥n:**
* **Preservaci√≥n del Significado:** A diferencia del **Stemming** (ej. `competitive` ‚Üí `competit`), que simplemente "corta" las palabras, la **Lematizaci√≥n** las reduce a su forma base de diccionario (lema), que es una palabra real con significado (ej. `finding` ‚Üí `find`). Esto es crucial para un an√°lisis sem√°ntico preciso.
* **Evita la Sobre-reducci√≥n:** Los procesos combinados ('d' y 'e') demostraron ser redundantes, ya que el *stemming* (la operaci√≥n m√°s agresiva) anula el beneficio de la lematizaci√≥n.
* **Eficiencia:** El lematizador de `spaCy` (usado en 'b') ya es contextual y utiliza informaci√≥n de **POS-Tagging** de forma inherente, haciendo que el paso expl√≠cito ('f') sea innecesario para este caso de uso.

---

## ‚òÅÔ∏è 3. Nube de Palabras (Punto 3)

Para validar visualmente la efectividad de nuestra normalizaci√≥n, se gener√≥ una **Nube de Palabras** (`WordCloud`) a partir del corpus lematizado.

Esta visualizaci√≥n confirma que el ruido (como "the", "a", "is") ha sido eliminado, y los t√©rminos m√°s frecuentes son ahora los sem√°nticamente relevantes para el tema.

**Top 10 T√©rminos del Corpus Lematizado:**
1.  `racing` (13)
2.  `line` (12)
3.  `car` (9)
4.  `track` (8)
... y m√°s.

*(A√±ade aqu√≠ el screenshot de tu nube de palabras)*
`![Nube de Palabras del Corpus 'Racing Lines'](wordcloud_racing_lines.png)`

---

## üî¢ 4. Vectorizaci√≥n de Documentos (Punto 4)

Este es el objetivo principal: transformar los 10 documentos de texto limpio en **vectores num√©ricos** para que puedan ser entendidos por un algoritmo. Se implementaron 4 t√©cnicas clave sobre un **vocabulario global de 144 t√©rminos**.

### a) One-Hot Encoding (Presencia de T√©rmino)
El m√©todo m√°s simple. Es un vector binario (0s y 1s) donde cada √≠ndice corresponde a una palabra del vocabulario.
* **1** = la palabra **est√° presente** en el documento.
* **0** = la palabra **no est√° presente**.
* **Limitaci√≥n:** Pierde toda la informaci√≥n de frecuencia. `car` apareciendo 1 vez o 10 veces da el mismo resultado (1).

### b) Conteo de T√©rminos (Bolsa de Palabras / Bag of Words)
Este vector almacena el **conteo de frecuencia** de cada palabra del vocabulario en el documento.
* *Ejemplo:* Si `car` aparece 3 veces en `doc_06`, el valor en ese √≠ndice ser√° `3`.
* **Limitaci√≥n:** Da demasiado peso a palabras que son muy comunes en *todos* los documentos (como `car` en este corpus), sesgando su importancia.

### c) Probabilidad del T√©rmino (P(t))
Esta t√©cnica crea un **√∫nico vector global** que describe la distribuci√≥n de probabilidad de los t√©rminos en todo el corpus.
* **F√≥rmula:** $ P(t) = \frac{\text{Frecuencia de } t \text{ en todo el corpus}}{\text{Total de t√©rminos en el corpus}} $
* **Uso:** No se usa para representar documentos individuales, sino para entender la composici√≥n del corpus en su conjunto.

### d) TF-IDF (Frecuencia de T√©rmino‚ÄìFrecuencia Inversa de Documento)
Es el m√©todo m√°s robusto para ponderar la importancia de un t√©rmino. Implementado desde cero, su l√≥gica es: **"Un t√©rmino es importante si es frecuente en *un* documento pero raro en *todos los dem√°s*."**



El puntaje se calcula en dos partes:
1.  **TF (Term Frequency):** Mide la importancia local de un t√©rmino en un documento.
    * $ TF(t, d) = \frac{\text{N¬∫ de veces que } t \text{ aparece en } d}{\text{Total de t√©rminos en } d} $
2.  **IDF (Inverse Document Frequency):** Mide la rareza del t√©rmino en todo el corpus.
    * $ IDF(t) = \log\left(\frac{\text{Total de documentos}}{\text{N¬∫ de documentos que contienen } t}\right) $

El puntaje final, **TF-IDF = TF \* IDF**, penaliza palabras comunes (como `car`) d√°ndoles un IDF bajo, y recompensa palabras espec√≠ficas (como `optimal` o `bezier`) con un IDF alto. Esto proporciona una representaci√≥n num√©rica mucho m√°s significativa de la "firma" sem√°ntica de cada documento.

---

## üöÄ C√≥mo Ejecutar

Este proyecto es un Jupyter Notebook (`.ipynb`) y requiere un entorno compatible.

### Requisitos
* Python 3.x
* Jupyter (Lab o Notebook)
* Las bibliotecas listadas en `requirements.txt`.
* El modelo de lenguaje `en_core_web_sm` de `spaCy`.

### Pasos de Instalaci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone [URL-DE-TU-REPOSITORIO]
    cd [NOMBRE-DEL-REPOSITORIO]
    ```

2.  **Instalar dependencias:**
    (Se recomienda crear un entorno virtual: `python -m venv venv` y `source venv/bin/activate`)
    ```bash
    pip install -r requirements.txt
    ```

3.  **Descargar el modelo de `spaCy` (¬°Importante!):**
    ```bash
    python -m spacy download en_core_web_sm
    ```

4.  **Iniciar Jupyter Lab:**
    ```bash
    jupyter lab
    ```

5.  Abrir el archivo `.ipynb` y ejecutar las celdas.

---

## üìÑ Contenido para `requirements.txt`
(Crea un archivo `requirements.txt` y pega esto)
```
spacy
nltk
pandas
matplotlib
wordcloud
```
