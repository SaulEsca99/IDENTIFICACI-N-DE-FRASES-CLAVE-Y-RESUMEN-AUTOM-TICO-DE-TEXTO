# üìä PR√ÅCTICA 3: IMPLEMENTACI√ìN Y EVALUACI√ìN DE NA√èVE BAYES
## Tecnolog√≠as de Lenguaje Natural

**Autor:** Escamilla Lazcano Sa√∫l
**Carrera:** Ingenier√≠a En Inteligencia Artificial

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![spaCy](https://img.shields.io/badge/Librer√≠a-spaCy-blue.svg)](https://spacy.io/)
[![NLTK](https://img.shields.io/badge/Librer√≠a-NLTK-green.svg)](https://www.nltk.org/)
[![Pandas](https://img.shields.io/badge/Librer√≠a-Pandas-purple.svg)](https://pandas.pydata.org/)
[![Scikit-learn](https://img.shields.io/badge/Librer√≠a-Scikit--learn-orange.svg)](https://scikit-learn.org/)

## üöÄ Descripci√≥n del Proyecto

Este proyecto es una implementaci√≥n completa del algoritmo **Clasificador Bayesiano Ingenuo (Na√Øve Bayes)** desde cero en Python, aplicado a un problema de **An√°lisis de Sentimiento**. El objetivo es predecir si una rese√±a de pel√≠cula es "positiva" o "negativa" bas√°ndose en su contenido textual.

El proyecto abarca todo el pipeline de un proyecto de PLN:
1.  **Carga y Exploraci√≥n de Datos** del dataset IMDB.
2.  **Preprocesamiento y Normalizaci√≥n de Texto** avanzado usando `spaCy` y `NLTK`.
3.  **Implementaci√≥n del Modelo** `NaiveBayesPersonalizado` desde cero.
4.  **Entrenamiento y Evaluaci√≥n** del modelo usando m√©tricas est√°ndar de clasificaci√≥n.
5.  **Visualizaci√≥n de Resultados**, incluyendo una matriz de confusi√≥n y nubes de palabras.

## üíæ 1. Dataset

El conjunto de datos utilizado es el **"IMDB Dataset of 50K Movie Reviews"**. Este es un corpus est√°ndar para tareas de clasificaci√≥n binaria de sentimiento.
* **Tama√±o:** 50,000 rese√±as.
* **Clases:** "positiva" (25,000) y "negativa" (25,000).
* **Objetivo:** Clasificar el sentimiento de la rese√±a.

## ‚öôÔ∏è 2. Pipeline de Preprocesamiento de Texto

Antes de entrenar, el texto crudo debe ser normalizado. Se implementaron dos m√©todos de normalizaci√≥n para comparar: *Stemming* (con `NLTK`) y *Lematizaci√≥n* (con `spaCy`).

Se seleccion√≥ la **Lematizaci√≥n** para el pipeline final, ya que produce palabras l√©xicamente correctas (lemas), lo que es m√°s preciso que las ra√≠ces generadas por el *stemming*.

El pipeline de normalizaci√≥n (`lematizar_texto`) incluye:
1.  **Conversi√≥n a Min√∫sculas:** `texto.lower()`
2.  **Eliminaci√≥n de HTML:** Se us√≥ `re` para eliminar etiquetas HTML (ej. `<br />`).
3.  **Tokenizaci√≥n (spaCy):** Se procesa el texto con el modelo `en_core_web_sm`.
4.  **Eliminaci√≥n de Stopwords y Puntuaci√≥n:** Se filtran palabras comunes y signos de puntuaci√≥n.
5.  **Lematizaci√≥n (spaCy):** Cada token se reduce a su forma base (ej. "running" ‚Üí "run").

## üß† 3. ImplementACI√ìN: Na√Øve Bayes desde Cero

El n√∫cleo de la pr√°ctica es la clase `NaiveBayesPersonalizado`, que no utiliza las implementaciones de `sklearn` para el clasificador.

### A. Entrenamiento (`fit`)

El m√©todo `fit` aprende las probabilidades necesarias del corpus de entrenamiento.

**1. C√°lculo de Priors de Clase $P(c)$:**
Se calcula la probabilidad de que un documento pertenezca a una clase (positiva o negativa) sin ver el texto.
$$ P(c) = \frac{\text{Documentos en la clase } c}{\text{Total de documentos}} $$

**2. C√°lculo de Probabilidades de Palabras (Likelihoods) $P(w|c)$:**
Se calcula la probabilidad de que una palabra $w$ aparezca, dado que pertenece a una clase $c$.

* **Conteo de Palabras:** Se construye un vocabulario de frecuencia para cada clase.
* **Suavizado de Laplace (Add-1):** Para manejar palabras que no se vieron en el entrenamiento (y evitar probabilidades de cero), se aplica el suavizado de Laplace ($ \alpha = 1 $).

La f√≥rmula para la probabilidad de una palabra con suavizado es:
$$ P(w_i | c) = \frac{\text{frecuencia}(w_i, c) + \alpha}{\text{Total de palabras en } c + \alpha \cdot |V|} $$
Donde $|V|$ es el tama√±o del vocabulario global.

### B. Predicci√≥n (`predict`)

El m√©todo `predict` clasifica nuevos documentos. Para evitar el *underflow* num√©rico (multiplicar muchas probabilidades peque√±as), se utiliza la suma de **log-probabilidades**:

$$ c_{\text{pred}} = \underset{c}{\operatorname{argmax}} \left( \log(P(c)) + \sum_{i=1}^{n} \log(P(w_i | c)) \right) $$

El documento se asigna a la clase $c$ que maximice esta suma.

## üìä 4. Evaluaci√≥n y Resultados

El modelo se entren√≥ con el 80% del dataset y se evalu√≥ con el 20% restante.

### M√©tricas de Desempe√±o
La evaluaci√≥n (`sklearn.metrics`) arroj√≥ excelentes resultados:

* **Accuracy (Exactitud):** ~86.2%
* **Reporte de Clasificaci√≥n:**
    | Clase | Precision | Recall | F1-Score |
    | :--- | :--- | :--- | :--- |
    | Negativa | 0.86 | 0.87 | 0.86 |
    | Positiva | 0.87 | 0.86 | 0.86 |

### Matriz de Confusi√≥n
La matriz de confusi√≥n (visualizada con `seaborn`) muestra c√≥mo se distribuyeron las predicciones correctas e incorrectas.



### Nubes de Palabras
Se generaron nubes de palabras (`wordcloud`) a partir del vocabulario aprendido por el modelo para las clases "positiva" y "negativa", mostrando los t√©rminos m√°s distintivos de cada sentimiento.

| Nube Positiva | Nube Negativa |
| :---: | :---: |
|  |  |

## üí° Conclusi√≥n

Esta pr√°ctica demostr√≥ con √©xito la implementaci√≥n de un clasificador Na√Øve Bayes Multinomial desde cero. Los resultados de exactitud (~86%) son muy buenos y demuestran la efectividad de este algoritmo para tareas de clasificaci√≥n de texto. El uso de un pipeline de normalizaci√≥n robusto (especialmente la lematizaci√≥n) y t√©cnicas como el suavizado de Laplace fueron cruciales para el rendimiento del modelo.

---

## üöÄ C√≥mo Ejecutar

Este proyecto es un Jupyter Notebook (`.ipynb`) y requiere un entorno compatible.

### Requisitos
* Python 3.x
* Jupyter (Lab o Notebook)
* Las bibliotecas listadas en `requirements.txt`
* El modelo de lenguaje `en_core_web_sm` de `spaCy`.
* El dataset `IMDB Dataset.csv` (no incluido en este repo, debe ser descargado).

### Pasos de Instalaci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone [URL-DE-TU-REPOSITORIO]
    cd [NOMBRE-DEL-REPOSITORIO]
    ```

2.  **Instalar dependencias:**
    (Se recomienda crear un entorno virtual: `python -m venv venv` y `source venv/bin/activate`)
    ```bash
    pip install -r requirements.txt
    ```

3.  **Descargar el modelo de `spaCy`:**
    ```bash
    python -m spacy download en_core_web_sm
    ```

4.  **Iniciar Jupyter Lab:**
    ```bash
    jupyter lab
    ```

5.  Abrir el archivo `.ipynb` y ejecutar las celdas.

---

## üìÑ Contenido para `requirements.txt`
(Crea un archivo `requirements.txt` y pega esto)
```
pandas
numpy
matplotlib
seaborn
wordcloud
nltk
spacy
scikit-learn
```