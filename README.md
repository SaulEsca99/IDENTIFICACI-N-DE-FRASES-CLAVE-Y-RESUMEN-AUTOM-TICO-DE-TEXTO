# üìä PR√ÅCTICA 3: IMPLEMENTACI√ìN Y EVALUACI√ìN DE NA√èVE BAYES
## Tecnolog√≠as de Lenguaje Natural

**Autor:** Escamilla Lazcano Sa√∫l
**Grupo:** 5BV1
**Carrera:** Ingenier√≠a En Inteligencia Artificial

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![Pandas](https://img.shields.io/badge/Librer√≠a-Pandas-purple.svg)](https://pandas.pydata.org/)
[![spaCy](https://img.shields.io/badge/Librer√≠a-spaCy-blue.svg)](https://spacy.io/)
[![NLTK](https://img.shields.io/badge/Librer√≠a-NLTK-green.svg)](https://www.nltk.org/)
[![Scikit-learn](https://img.shields.io/badge/Librer√≠a-Scikit--learn-orange.svg)](https://scikit-learn.org/)
[![Seaborn](https://img.shields.io/badge/Librer√≠a-Seaborn%20%7C%20Matplotlib-blueviolet.svg)](https://seaborn.pydata.org/)

## üöÄ Descripci√≥n del Proyecto

Este proyecto es una implementaci√≥n completa del algoritmo clasificador **Bayesiano Ingenuo (Na√Øve Bayes)** **desde cero** en Python. El objetivo es construir un modelo de **An√°lisis de Sentimiento** capaz de predecir si una rese√±a de pel√≠cula es "positiva" o "negativa" bas√°ndose √∫nicamente en su contenido textual.

El *pipeline* del proyecto cubre todos los pasos esenciales de una tarea de PLN:
1.  **Carga y Exploraci√≥n de Datos** del dataset IMDB.
2.  **Preprocesamiento y Normalizaci√≥n de Texto** avanzado usando `spaCy` y `NLTK`.
3.  **Implementaci√≥n del Modelo** (`NaiveBayesPersonalizado`) desde cero.
4.  **Entrenamiento y Evaluaci√≥n** del modelo con m√©tricas de clasificaci√≥n est√°ndar.
5.  **Visualizaci√≥n de Resultados**, incluyendo una matriz de confusi√≥n y nubes de palabras.

## üíæ 1. Dataset

El conjunto de datos utilizado es el **"IMDB Dataset of 50K Movie Reviews"**. Este es un corpus can√≥nico para tareas de clasificaci√≥n binaria de sentimiento.
* **Archivo:** `IMDB Dataset.csv`
* **Tama√±o:** 50,000 rese√±as.
* **Clases:** "positiva" (25,000) y "negativa" (25,000).

## ‚öôÔ∏è 2. Pipeline de Preprocesamiento de Texto

Antes de entrenar, el texto crudo debe ser normalizado. Se implementaron dos m√©todos de normalizaci√≥n para comparar: *Stemming* (con `NLTK`) y *Lematizaci√≥n* (con `spaCy`).

Se seleccion√≥ la **Lematizaci√≥n** para el pipeline final, ya que produce palabras l√©xicamente correctas (lemas), lo que es m√°s preciso que las ra√≠ces generadas por el *stemming*.

El pipeline de normalizaci√≥n (`lematizar_texto`) incluye:
1.  **Conversi√≥n a Min√∫sculas:** `texto.lower()`
2.  **Eliminaci√≥n de HTML:** Se us√≥ `re` para eliminar etiquetas (ej. `<br />`).
3.  **Tokenizaci√≥n (spaCy):** Se procesa el texto con el modelo `en_core_web_sm`.
4.  **Eliminaci√≥n de Stopwords y Puntuaci√≥n:** Se filtran palabras comunes y signos de puntuaci√≥n.
5.  **Lematizaci√≥n (spaCy):** Cada token se reduce a su forma base de diccionario (ej. "running" ‚Üí "run").

## üß† 3. Implementaci√≥n: Na√Øve Bayes desde Cero

El n√∫cleo de la pr√°ctica es la clase `NaiveBayesPersonalizado`, que no utiliza las implementaciones de `sklearn` para el clasificador.

### A. Entrenamiento (`fit`)
El m√©todo `fit` aprende las probabilidades necesarias del corpus de entrenamiento (`X_train`, `y_train`).

**1. C√°lculo de Priors de Clase $P(c)$:**
Calcula la probabilidad base de cada clase (positiva o negativa) en el dataset.
$$ P(c) = \frac{\text{Documentos en la clase } c}{\text{Total de documentos}} $$

**2. C√°lculo de Probabilidades Condicionales (Likelihoods) $P(w|c)$:**
Calcula la probabilidad de que una palabra $w$ aparezca, dado que pertenece a una clase $c$.

* **Conteo de Palabras:** Se construye un vocabulario de frecuencia para cada clase.
* **Suavizado de Laplace (Add-1):** Se aplica un suavizado (con $\alpha = 1$) para manejar palabras que aparecen en el set de prueba pero no en el de entrenamiento. Esto evita probabilidades de cero que anular√≠an todo el c√°lculo.

La f√≥rmula de probabilidad de una palabra con suavizado es:
$$ P(w_i | c) = \frac{\text{frecuencia}(w_i, c) + \alpha}{\text{Total de palabras en } c + \alpha \cdot |V|} $$
Donde $|V|$ es el tama√±o del vocabulario global.

### B. Predicci√≥n (`predict`)
El m√©todo `predict` clasifica nuevos documentos. Para evitar el **underflow num√©rico** (multiplicar muchas probabilidades peque√±as da como resultado cero), se utiliza la **suma de log-probabilidades**. El teorema de Bayes en su forma logar√≠tmica es:

$$ c_{\text{pred}} = \underset{c}{\operatorname{argmax}} \left( \log(P(c)) + \sum_{i=1}^{n} \log(P(w_i | c)) \right) $$

El modelo asigna la clase $c$ (positiva o negativa) que maximice esta suma.



## üìä 4. Evaluaci√≥n y Resultados

El modelo se entren√≥ con el 80% de los datos y se evalu√≥ con el 20% restante.

### M√©tricas de Desempe√±o
La evaluaci√≥n (`sklearn.metrics`) arroj√≥ un rendimiento excelente:

* **Accuracy (Exactitud):** **~86.2%**
* **Reporte de Clasificaci√≥n:**
    | Clase | Precision | Recall | F1-Score |
    | :--- | :--- | :--- | :--- |
    | Negativa | 0.86 | 0.87 | 0.86 |
    | Positiva | 0.87 | 0.86 | 0.86 |

### Matriz de Confusi√≥n
La matriz de confusi√≥n (visualizada con `seaborn`) confirma el buen desempe√±o del modelo, mostrando una alta concentraci√≥n de predicciones correctas en la diagonal principal.

*(**Instrucci√≥n:** Sube tu imagen de la matriz de confusi√≥n al repositorio y n√≥mbrala `matriz_confusion.png` para que aparezca aqu√≠)*
`![Matriz de Confusi√≥n](matriz_confusion.png)`

### Nubes de Palabras
Se generaron nubes de palabras (`wordcloud`) a partir de los vocabularios aprendidos por el modelo para cada clase, mostrando visualmente los t√©rminos m√°s distintivos de cada sentimiento.

*(**Instrucci√≥n:** Sube tus nubes de palabras y n√≥mbralas como se sugiere)*
| Nube de Palabras Positivas | Nube de Palabras Negativas |
| :---: | :---: |
| `![Nube de Palabras Positivas](wordcloud_positiva.png)` | `![Nube de Palabras Negativas](wordcloud_negativa.png)` |

## üí° Conclusi√≥n

Esta pr√°ctica demostr√≥ con √©xito la implementaci√≥n de un clasificador Na√Øve Bayes Multinomial desde cero. Los resultados de exactitud (~86%) son muy buenos y demuestran la efectividad de este algoritmo para tareas de clasificaci√≥n de texto. El uso de un pipeline de normalizaci√≥n robusto (especialmente la lematizaci√≥n) y t√©cnicas como el suavizado de Laplace fueron cruciales para el rendimiento del modelo.

---

## üöÄ C√≥mo Ejecutar

Este proyecto es un Jupyter Notebook (`.ipynb`) y requiere un entorno compatible.

### Requisitos
* Python 3.x
* Jupyter (Lab o Notebook)
* Las bibliotecas listadas en `requirements.txt`.
* El modelo de lenguaje `en_core_web_sm` de `spaCy`.
* **Importante:** El archivo del dataset `IMDB Dataset.csv` debe estar en la misma carpeta.

### Pasos de Instalaci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone [URL-DE-TU-REPOSITORIO]
    cd [NOMBRE-DEL-REPOSITORIO]
    ```

2.  **Instalar dependencias:**
    (Se recomienda crear un entorno virtual: `python -m venv venv` y `source venv/bin/activate`)
    ```bash
    pip install -r requirements.txt
    ```

3.  **Descargar el modelo de `spaCy`:**
    ```bash
    python -m spacy download en_core_web_sm
    ```

4.  **Iniciar Jupyter Lab:**
    ```bash
    jupyter lab
    ```

5.  Abrir el archivo `Practica3_EscamillaLazcanoSaul_5BV1.ipynb` y ejecutar las celdas.

### üêõ Soluci√≥n de Errores Comunes

**Error (el de tu imagen):** `ModuleNotFoundError: No module named 'wordcloud'`

**Soluci√≥n:** Este error significa que la biblioteca `wordcloud` no est√° instalada en tu entorno. Para arreglarlo:

1.  Abre tu terminal.
2.  Activa tu entorno de Conda (ej. `conda activate nlp_env`).
3.  Ejecuta el siguiente comando:
    ```bash
    conda install -c conda-forge wordcloud
    ```
4.  **Reinicia el kernel** de tu Jupyter Notebook y vuelve a ejecutar las celdas.

---

## üìÑ Contenido para `requirements.txt`
(Crea un archivo `requirements.txt` y pega esto)
```
pandas
numpy
matplotlib
seaborn
wordcloud
nltk
spacy
scikit-learn
```
